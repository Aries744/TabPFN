{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#from scripts.differentiable_pfn_evaluation import eval_model_range\n",
    "# No string\n",
    "from scripts.model_builder import get_model, get_default_spec, save_model, load_model\n",
    "from scripts.transformer_prediction_interface import transformer_predict, get_params_from_config, load_model_workflow\n",
    "\n",
    "from scripts.model_configs import *\n",
    "\n",
    "from datasets import load_openml_list, open_cc_dids, open_cc_valid_dids\n",
    "from priors.utils import plot_prior, plot_features\n",
    "from priors.utils import uniform_int_sampler_f\n",
    "\n",
    "from scripts.tabular_metrics import calculate_score_per_method, calculate_score\n",
    "# No string\n",
    "#from scripts.tabular_evaluation import evaluate\n",
    "\n",
    "from priors.differentiable_prior import DifferentiableHyperparameterList, draw_random_style, merge_style_with_info\n",
    "from scripts import tabular_metrics\n",
    "from notebook_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_datasets = True\n",
    "max_samples = 10000 if large_datasets else 5000\n",
    "bptt = 10000 if large_datasets else 3000\n",
    "suite='cc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "base_path = '.'\n",
    "max_features = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_models(model_string):\n",
    "    print(model_string)\n",
    "\n",
    "    for i in range(80):\n",
    "        for e in range(50):\n",
    "            exists = Path(os.path.join(base_path, f'models_diff/prior_diff_real_checkpoint{model_string}_n_{i}_epoch_{e}.cpkt')).is_file()\n",
    "            if exists:\n",
    "                print(os.path.join(base_path, f'models_diff/prior_diff_real_checkpoint{model_string}_n_{i}_epoch_{e}.cpkt'))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_function(config_sample, i, add_name=''):\n",
    "    start_time = time.time()\n",
    "    N_epochs_to_save = 50\n",
    "    \n",
    "    def save_callback(model, epoch):\n",
    "        if not hasattr(model, 'last_saved_epoch'):\n",
    "            model.last_saved_epoch = 0\n",
    "        if ((time.time() - start_time) / (maximum_runtime * 60 / N_epochs_to_save)) > model.last_saved_epoch:\n",
    "            print('Saving model..')\n",
    "            config_sample['epoch_in_training'] = epoch\n",
    "            save_model(model, base_path, f'models_diff/prior_diff_real_checkpoint{add_name}_n_{i}_epoch_{model.last_saved_epoch}.cpkt',\n",
    "                           config_sample)\n",
    "            model.last_saved_epoch = model.last_saved_epoch + 1 # TODO: Rename to checkpoint\n",
    "    \n",
    "    model = get_model(config_sample\n",
    "                      , device\n",
    "                      , should_train=True\n",
    "                      , verbose=1\n",
    "                      , epoch_callback = save_callback)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define prior settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reload_config(config_type='causal', task_type='multiclass', longer=0):\n",
    "    config = get_prior_config(config_type=config_type)\n",
    "    \n",
    "    config['prior_type'], config['differentiable'], config['flexible'] = 'mlp', True, True\n",
    "    \n",
    "    model_string = ''\n",
    "    \n",
    "    config['epochs'] = 12000\n",
    "    config['recompute_attn'] = False\n",
    "\n",
    "    config['max_num_classes'] = 10\n",
    "    config['num_classes'] = uniform_int_sampler_f(2, config['max_num_classes'])\n",
    "    config['balanced'] = False\n",
    "    model_string = model_string + '_multiclass'\n",
    "    \n",
    "    model_string = model_string + '_'+datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "    \n",
    "    return config, model_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualize Prior samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config, model_string = reload_config(longer=1)\n",
    "\n",
    "config['bptt_extra_samples'] = None\n",
    "\n",
    "# diff\n",
    "config['output_multiclass_ordered_p'] = 0.\n",
    "del config['differentiable_hyperparameters']['output_multiclass_ordered_p']\n",
    "\n",
    "config['multiclass_type'] = 'rank'\n",
    "del config['differentiable_hyperparameters']['multiclass_type']\n",
    "\n",
    "config['sampling'] = 'normal' # vielleicht schlecht?\n",
    "del config['differentiable_hyperparameters']['sampling']\n",
    "\n",
    "config['pre_sample_causes'] = True\n",
    "# end diff\n",
    "\n",
    "config['multiclass_loss_type'] = 'nono' # 'compatible'\n",
    "config['normalize_to_ranking'] = False # False\n",
    "\n",
    "config['categorical_feature_p'] = .2 # diff: .0\n",
    "\n",
    "# turn this back on in a random search!?\n",
    "config['nan_prob_no_reason'] = .0\n",
    "config['nan_prob_unknown_reason'] = .0 # diff: .0\n",
    "config['set_value_to_nan'] = .1 # diff: 1.\n",
    "\n",
    "config['normalize_with_sqrt'] = False\n",
    "\n",
    "config['new_mlp_per_example'] = True\n",
    "config['prior_mlp_scale_weights_sqrt'] = True\n",
    "config['batch_size_per_gp_sample'] = None\n",
    "\n",
    "config['normalize_ignore_label_too'] = False\n",
    "\n",
    "config['differentiable_hps_as_style'] = False\n",
    "config['max_eval_pos'] = 1000\n",
    "\n",
    "config['random_feature_rotation'] = True\n",
    "config['rotate_normalized_labels'] = True\n",
    "\n",
    "config[\"mix_activations\"] = False # False heisst eig True\n",
    "\n",
    "config['emsize'] = 512 # 512 in the paper\n",
    "config['emsize_f'] = 100\n",
    "config['nhead'] = config['emsize'] // 8 \n",
    "config['bptt'] = 1024+128\n",
    "config['canonical_y_encoder'] = False\n",
    "\n",
    "    \n",
    "config['aggregate_k_gradients'] = 8\n",
    "config['batch_size'] = 1 * config['aggregate_k_gradients']\n",
    "config['num_steps'] = 1024//config['aggregate_k_gradients']\n",
    "config['epochs'] = 400\n",
    "config['total_available_time_in_s'] = None #60*60*22 # 22 hours for some safety...\n",
    "\n",
    "config['train_mixed_precision'] = True\n",
    "config['efficient_eval_masking'] = True\n",
    "\n",
    "# Ugne\n",
    "# config[\"normalize_by_used_features\"] = False\n",
    "config['categorical_feature_p'] = 0.7\n",
    "\n",
    "config_sample = evaluate_hypers(config)\n",
    "\n",
    "# print(config_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "config_sample['batch_size'] = 4\n",
    "model = get_model(config_sample, device, should_train=False, verbose=2) # , state_dict=model[2].state_dict()\n",
    "(hp_embedding, data, _), targets, single_eval_pos = next(iter(model[3]))\n",
    "\n",
    "from utils import normalize_data\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "N = 100\n",
    "plot_features(data[0:N, 0, 0:4], targets[0:N, 0], fig=fig)\n",
    "\n",
    "d = np.concatenate([data[:, 0, :].T, np.expand_dims(targets[:, 0], -1).T])\n",
    "d[np.isnan(d)] = 0\n",
    "c = np.corrcoef(d)\n",
    "plt.matshow(np.abs(c), vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ugne: we use priors.flexible_categorical (model_builder.py)\n",
      "Using style prior: True\n",
      "Using cpu:0 device\n",
      "Ugne: initial x torch.Size([1152, 1, 87]) (flexible_categorical.py)\n",
      "Ugne: initial x tensor([[[ 0.0945, -1.6938, -2.4606,  ...,  0.9249, -1.1252, -1.8490]],\n",
      "\n",
      "        [[ 0.0857,  0.2226, -0.9188,  ...,  0.8294, -1.2725,  0.3050]],\n",
      "\n",
      "        [[ 0.0141, -0.4122, -1.0279,  ...,  1.0499, -1.5080, -1.2065]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0422, -1.0256, -1.8662,  ...,  0.5562, -1.5768, -2.4965]],\n",
      "\n",
      "        [[ 0.0667, -0.3806, -0.3929,  ...,  1.3083, -2.5427, -1.6829]],\n",
      "\n",
      "        [[ 0.0688, -0.4434, -1.0777,  ...,  1.4759, -1.5713, -3.1228]]]) (flexible_categorical.py)\n",
      "Ugne: self.h['categorical_feature_p'] 0.2 (flexible_categorical.py)\n",
      "Ugne: Categorical features if statement is true and we have p=0.3464065073710696 (flexible_categorical.py)\n",
      "Ugne: x.shape[2] 87 (flexible_categorical.py)\n",
      "Ugne: Categorical features if if statement is True and we have x torch.Size([1152, 1, 87]) (flexible_categorical.py)\n",
      "Ugne: Categorical features if if statement is True and we have x tensor([[[1., 0., 1.,  ..., 1., 0., 1.]],\n",
      "\n",
      "        [[1., 1., 0.,  ..., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 1., 1.,  ..., 1., 0., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 0., 1.,  ..., 0., 0., 1.]],\n",
      "\n",
      "        [[1., 1., 0.,  ..., 1., 0., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 0., 1.]]]) (flexible_categorical.py)\n",
      "Ugne: normalize_by_used_features = True but I'm setting to False manually\n",
      "Ugne: x after normalize_by_used_features torch.Size([1152, 1, 87])\n",
      "Ugne: x after normalize_by_used_features tensor([[[1., 0., 1.,  ..., 1., 0., 1.]],\n",
      "\n",
      "        [[1., 1., 0.,  ..., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 1., 1.,  ..., 1., 0., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 0., 1.,  ..., 0., 0., 1.]],\n",
      "\n",
      "        [[1., 1., 0.,  ..., 1., 0., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 0., 1.]]])\n",
      "Ugne: x output torch.Size([1152, 1, 100])\n",
      "Ugne: x output tensor([[[1., 0., 1.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 1.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 0., 1.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 0., 0., 0.]]])\n",
      "Ugne: get_batch() output x torch.Size([1152, 1, 100])\n",
      "Ugne: get_batch() output x tensor([[[1., 0., 1.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 1.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 0., 1.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 0., 0., 0.]]])\n",
      "Style definition of first 3 examples: None\n",
      "Ugne: style_encoder_generator <class 'encoders.StyleEncoder'> (train.py)\n",
      "Ugne: style_encoder None (train.py)\n",
      "Using a Transformer with 14.84 M parameters\n",
      "Ugne: initial x torch.Size([1152, 1, 84]) (flexible_categorical.py)\n",
      "Ugne: initial x tensor([[[  176.4851,   327.6037,  -810.6679,  ...,   -83.7332,\n",
      "            105.5467,    46.0793]],\n",
      "\n",
      "        [[ -881.7738,  1380.3794, -1292.0768,  ...,  -329.3773,\n",
      "            568.2640,    11.5173]],\n",
      "\n",
      "        [[ -226.0897,   502.1413,  -202.2376,  ...,   -89.1596,\n",
      "            224.2506,  -116.3919]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  443.9386,    91.2678,   461.3457,  ...,   -87.8112,\n",
      "            260.4440,  -531.9300]],\n",
      "\n",
      "        [[  471.5861,   143.8531,    67.1641,  ...,   -99.1077,\n",
      "            232.4761,  -409.3350]],\n",
      "\n",
      "        [[ -443.2215,   953.4839, -1602.0068,  ...,  -191.6938,\n",
      "            255.8781,   280.6591]]]) (flexible_categorical.py)\n",
      "Ugne: self.h['categorical_feature_p'] 0.2 (flexible_categorical.py)\n",
      "Ugne: normalize_by_used_features = True but I'm setting to False manually\n",
      "Ugne: x after normalize_by_used_features torch.Size([1152, 1, 84])\n",
      "Ugne: x after normalize_by_used_features tensor([[[  176.4851,   327.6037,  -810.6679,  ...,   -83.7332,\n",
      "            105.5467,    46.0793]],\n",
      "\n",
      "        [[ -881.7738,  1380.3794, -1292.0768,  ...,  -329.3773,\n",
      "            568.2640,    11.5173]],\n",
      "\n",
      "        [[ -226.0897,   502.1413,  -202.2376,  ...,   -89.1596,\n",
      "            224.2506,  -116.3919]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  443.9386,    91.2678,   461.3457,  ...,   -87.8112,\n",
      "            260.4440,  -531.9300]],\n",
      "\n",
      "        [[  471.5861,   143.8531,    67.1641,  ...,   -99.1077,\n",
      "            232.4761,  -409.3350]],\n",
      "\n",
      "        [[ -443.2215,   953.4839, -1602.0068,  ...,  -191.6938,\n",
      "            255.8781,   280.6591]]])\n",
      "Ugne: x output torch.Size([1152, 1, 100])\n",
      "Ugne: x output tensor([[[  176.4851,   327.6037,  -810.6679,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        [[ -881.7738,  1380.3794, -1292.0768,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        [[ -226.0897,   502.1413,  -202.2376,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  443.9386,    91.2678,   461.3457,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        [[  471.5861,   143.8531,    67.1641,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        [[ -443.2215,   953.4839, -1602.0068,  ...,     0.0000,\n",
      "              0.0000,     0.0000]]])\n",
      "Ugne: get_batch() output x torch.Size([1152, 1, 100])\n",
      "Ugne: get_batch() output x tensor([[[  176.4851,   327.6037,  -810.6679,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        [[ -881.7738,  1380.3794, -1292.0768,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        [[ -226.0897,   502.1413,  -202.2376,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  443.9386,    91.2678,   461.3457,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        [[  471.5861,   143.8531,    67.1641,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        [[ -443.2215,   953.4839, -1602.0068,  ...,     0.0000,\n",
      "              0.0000,     0.0000]]])\n",
      "This is the size of x_src:torch.Size([1152, 1, 100])\n",
      "This is the size of x_src:tensor([[[  176.4851,   327.6037,  -810.6679,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        [[ -881.7738,  1380.3794, -1292.0768,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        [[ -226.0897,   502.1413,  -202.2376,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  443.9386,    91.2678,   461.3457,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        [[  471.5861,   143.8531,    67.1641,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        [[ -443.2215,   953.4839, -1602.0068,  ...,     0.0000,\n",
      "              0.0000,     0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "model = get_model(config_sample, device, should_train=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0q/4mgwbnlj0tj34_6lyhzrbs5c0000gn/T/ipykernel_4835/1906148107.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_eval_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "input = (torch.rand([100, 10]), torch.rand([100, 1]))\n",
    "\n",
    "model(input, single_eval_point = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e0db07081617c9a6eca3dcd9ae8397d12b2119e3e97abe44125c2c5f990a5fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
