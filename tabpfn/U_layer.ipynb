{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.modules.transformer import _get_activation_fn, Module, Tensor, Optional, MultiheadAttention, Linear, Dropout, LayerNorm\n",
    "from einops import rearrange\n",
    "\n",
    "from layer import TransformerEncoderLayer\n",
    "from layer_new import *\n",
    "from encoders import * \n",
    "from transformer import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passed into train() in train.py\n",
    "emsize=512 #yes, same in the paper\n",
    "nhead=4 #yes, same in the paper\n",
    "nhid=2*emsize # #yes, same in the paper: 1024\n",
    "nlayers=1 # nlayers=6 # hmm, paper says 12\n",
    "\n",
    "# my param:\n",
    "num_features=20\n",
    "\n",
    "# for TransformerEncoderLayer()\n",
    "d_model=emsize\n",
    "nhead=nhead\n",
    "\n",
    "encoder = StyleEncoder(num_features, emsize)\n",
    "y_encoder = StyleEmbEncoder(1, emsize)\n",
    "style_encoder = None\n",
    "global_att_embeddings = None \n",
    "full_attention = False\n",
    "efficient_eval_masking = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## layer - manually"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = (torch.rand([100,20]),torch.randint(0,10,[100,1])) \n",
    "single_eval_pos = 78\n",
    "\n",
    "src=input\n",
    "src_mask=single_eval_pos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = TransformerEncoderLayer(d_model=emsize, nhead=nhead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 512])\n"
     ]
    }
   ],
   "source": [
    "# (1)\n",
    "assert isinstance(src, tuple), 'inputs (src) have to be given as (x,y) or (style,x,y) tuple'\n",
    "\n",
    "# (2)\n",
    "if len(src) == 2: # (x,y) and no style\n",
    "    src = (None,) + src\n",
    "\n",
    "# (3)\n",
    "style_src, x_src, y_src = src\n",
    "# (4)\n",
    "x_src = encoder(x_src)\n",
    "y_src = y_encoder(y_src.unsqueeze(-1) if len(y_src.shape) < len(x_src.shape) else y_src)\n",
    "style_src = style_encoder(style_src).unsqueeze(0) if style_encoder else \\\n",
    "    torch.tensor([], device=x_src.device)\n",
    "global_src = torch.tensor([], device=x_src.device) if global_att_embeddings is None else \\\n",
    "    global_att_embeddings.weight.unsqueeze(1).repeat(1, x_src.shape[1], 1)\n",
    "\n",
    "if src_mask is not None: assert global_att_embeddings is None or isinstance(src_mask, tuple)\n",
    "if src_mask is None: # this is RUN: default src_mask=None not changed it seems\n",
    "    if global_att_embeddings is None: # this is RUN: global_att_embeddings=None it seems\n",
    "        # (5)\n",
    "        full_len = len(x_src) + len(style_src)\n",
    "        if full_attention: # NOT RUN: full_attention=False\n",
    "            src_mask = bool_mask_to_att_mask(torch.ones((full_len, full_len), dtype=torch.bool)).to(x_src.device)\n",
    "        elif efficient_eval_masking: # this is RUN: efficient_eval_masking=True\n",
    "            src_mask = single_eval_pos + len(style_src)\n",
    "        else:\n",
    "            src_mask = generate_D_q_matrix(full_len, len(x_src) - single_eval_pos).to(x_src.device)\n",
    "    else:\n",
    "        src_mask_args = (global_att_embeddings.num_embeddings,\n",
    "                            len(x_src) + len(style_src),\n",
    "                            len(x_src) + len(style_src) - single_eval_pos)\n",
    "        src_mask = (generate_global_att_globaltokens_matrix(*src_mask_args).to(x_src.device),\n",
    "                    generate_global_att_trainset_matrix(*src_mask_args).to(x_src.device),\n",
    "                    generate_global_att_query_matrix(*src_mask_args).to(x_src.device))\n",
    "# (6)\n",
    "train_x = x_src[:single_eval_pos] + y_src[:single_eval_pos]\n",
    "# (7)\n",
    "src = torch.cat([global_src, style_src, train_x, x_src[single_eval_pos:]], 0)\n",
    "\n",
    "print(src.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-611e34b1216b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msingle_eval_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/DL_project/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Geneve/UNIGE/courses/[S3] Fall 2022/Deep Learning/project/TabPFN/tabpfn/layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m# AssertionError when src_key_padding_mask=None --> so src_key_padding_mask must be not None (but it is None - default None is not changed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0msingle_eval_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0msrc_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0msrc_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0msrc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msrc_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_right\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DL_project/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DL_project/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m   1036\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DL_project/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4947\u001b[0m     \u001b[0;31m# set up shape vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4948\u001b[0;31m     \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4949\u001b[0m     \u001b[0msrc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4950\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0membed_dim_to_check\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "output = encoder_layer(src=src, src_mask=single_eval_pos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(10,100,emsize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100, 512])\n",
      "tensor([[[ 1.3044,  0.4125, -1.3854,  ..., -0.3428,  0.0858,  1.7267],\n",
      "         [ 0.4566, -0.7945,  0.2055,  ...,  0.5292, -1.2625,  1.2702],\n",
      "         [ 1.4319,  1.4634,  1.1549,  ..., -0.1578, -1.3186,  0.5462],\n",
      "         ...,\n",
      "         [ 0.5837, -1.1374,  1.1215,  ...,  0.1268, -0.7942, -0.7186],\n",
      "         [ 0.0514,  1.0702,  0.3732,  ..., -1.0067, -0.4521,  1.0087],\n",
      "         [ 1.5101, -0.4314,  0.5100,  ..., -0.9074, -0.0667,  1.3394]],\n",
      "\n",
      "        [[ 0.1783,  0.8957, -0.7370,  ..., -0.5013,  0.2503,  0.6662],\n",
      "         [ 0.9626,  0.5682, -0.2310,  ..., -1.0270, -0.7095, -0.5113],\n",
      "         [ 1.0692,  0.2713,  0.5007,  ...,  0.5971,  0.8838,  0.3847],\n",
      "         ...,\n",
      "         [ 1.3070,  0.1561,  0.9569,  ..., -1.0775,  1.3538,  1.2830],\n",
      "         [ 1.8336, -0.5232, -1.5707,  ..., -0.1876, -1.0324, -0.4413],\n",
      "         [ 1.2467,  1.6991, -0.2167,  ...,  0.2264,  1.4520,  0.9081]],\n",
      "\n",
      "        [[ 1.6673,  0.9933, -1.4003,  ...,  0.1119,  1.1545,  2.0656],\n",
      "         [ 0.3479,  0.3006,  0.1022,  ..., -1.3856,  0.1448,  1.0510],\n",
      "         [-0.0733,  0.9752,  0.3379,  ..., -0.6145, -0.1917, -0.0344],\n",
      "         ...,\n",
      "         [-0.6211,  0.5673, -0.5744,  ..., -0.1447,  0.6646,  2.0743],\n",
      "         [ 0.3678,  0.5078, -1.2224,  ...,  1.0249, -1.2155,  2.2041],\n",
      "         [-0.4862,  0.9253,  0.2590,  ..., -0.5374,  1.4919,  1.9144]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1580,  1.1744, -0.0525,  ..., -0.2767, -0.6101,  1.5177],\n",
      "         [ 1.7786, -0.9156, -0.3081,  ...,  0.5856,  0.7018, -0.9953],\n",
      "         [ 1.1891, -0.8612,  1.0928,  ..., -0.8354, -1.1302,  0.4576],\n",
      "         ...,\n",
      "         [ 0.1693, -0.9883,  0.4702,  ..., -1.2125, -0.0059, -0.1136],\n",
      "         [ 1.3640,  1.2055, -1.8753,  ...,  1.4830, -1.1525, -0.4741],\n",
      "         [ 1.6845,  0.5115, -1.4771,  ...,  1.4514, -0.3451,  1.9654]],\n",
      "\n",
      "        [[ 1.5526, -0.8861,  0.8134,  ...,  0.7491,  0.8689,  0.6301],\n",
      "         [ 0.0955,  0.1603,  0.7431,  ..., -1.1394,  1.0390,  0.4511],\n",
      "         [ 0.1196, -0.3621,  0.3107,  ..., -0.3221,  0.2669,  0.9417],\n",
      "         ...,\n",
      "         [ 1.7342,  1.1567,  0.4642,  ...,  0.5482,  0.3390,  0.5703],\n",
      "         [ 0.1024,  0.1966, -0.5738,  ...,  0.6200, -1.6256,  0.6010],\n",
      "         [ 2.0969, -1.1251,  0.2269,  ...,  1.1525, -1.1776, -0.6168]],\n",
      "\n",
      "        [[ 0.1245,  1.1371, -0.5796,  ..., -1.1533, -0.0165,  0.1639],\n",
      "         [ 1.4909,  1.5328, -1.5718,  ...,  0.7153,  0.1923,  1.0014],\n",
      "         [ 0.6853,  0.7167, -1.1812,  ..., -0.6932,  0.3977,  2.0801],\n",
      "         ...,\n",
      "         [-0.3739,  0.6324, -0.7478,  ...,  0.7728, -1.1958, -0.2907],\n",
      "         [ 2.0659,  0.2296, -0.1392,  ..., -1.2388, -1.4580,  0.9921],\n",
      "         [-0.0665, -0.7230, -0.5929,  ..., -0.7006,  0.2478,  0.6780]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = TransformerEncoderLayer(d_model=emsize, nhead=nhead)\n",
    "output = encoder_layer(input, src_mask=78)\n",
    "\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-b2d312cfdd94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mencoder_layer_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerEncoderLayer_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_layer_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m78\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DL_project/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Geneve/UNIGE/courses/[S3] Fall 2022/Deep Learning/project/TabPFN/tabpfn/layer_new.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0msrc_right_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_linear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# <- linear layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0msrc_left_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minter_feature_attn_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_left_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_left_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_left_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0msrc_right_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minter_feature_attn_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_right_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_right_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_right_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# <- interfeature attnetion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DL_project/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DL_project/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m   1036\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DL_project/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4947\u001b[0m     \u001b[0;31m# set up shape vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4948\u001b[0;31m     \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4949\u001b[0m     \u001b[0msrc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4950\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0membed_dim_to_check\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "encoder_layer_new = TransformerEncoderLayer_new(d_model=emsize, nhead=nhead)\n",
    "output_new = encoder_layer_new(input, src_mask=78)\n",
    "\n",
    "print(output_new.shape)\n",
    "print(output_new)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer NEW 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=512\n",
    "dropout=0.0\n",
    "device=None\n",
    "dtype=None\n",
    "\n",
    "batch_first=False\n",
    "factory_kwargs = {'device': device, 'dtype': dtype}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first)\n",
    "\n",
    "# Implementation of Feedforward model\n",
    "        \n",
    "############################## Inter-feature attention ############################################\n",
    "pre_linear1 = Linear(1, d_model)\n",
    "pre_linear2 = Linear(1, d_model)\n",
    "\n",
    "pre_linear3 = Linear(d_model, 1)\n",
    "pre_linear4 = Linear(d_model, 1)\n",
    "\n",
    "inter_feature_attn_1 = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                    **factory_kwargs)\n",
    "inter_feature_attn_2 = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                    **factory_kwargs)\n",
    "####################################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "# src = torch.rand(2000,emsize) # input that's originally comming \n",
    "src = torch.rand(2000,100)\n",
    "src_mask = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_ = src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "single_eval_position = src_mask\n",
    "            \n",
    "src1 = src_.unsqueeze(-1)\n",
    "\n",
    "# Linear Layers()\n",
    "# Multihead attention for the right and left\n",
    "\n",
    "################### The interfeature implementation ###########################\n",
    "src_left_ = pre_linear1(src1[:single_eval_position]) # \n",
    "src_right_ = pre_linear2(src1[single_eval_position:]) # <- linear layers\n",
    "            \n",
    "src_left_ = inter_feature_attn_1(src_left_, src_left_, src_left_)[0] #\n",
    "src_right_ = inter_feature_attn_2(src_right_, src_right_, src_right_)[0] # <- interfeature attnetion\n",
    "\n",
    "src_left_ = pre_linear3(src_left_) # \n",
    "src_right_ = pre_linear4(src_right_) # <- linear layers to squeeze everything back up\n",
    "\n",
    "src_left_ = torch.squeeze(src_left_)\n",
    "src_right_ = torch.squeeze(src_right_, -1) # <- set it back to usual [points, features] tensor\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "src_left = self_attn(src_left_, src_left_, src_left_)[0]\n",
    "src_right = self_attn(src_right_, src_left_, src_left_)[0]\n",
    "\n",
    "src2 = torch.cat([src_left, src_right], dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "single_eval_position = src_mask\n",
    "\n",
    "print(single_eval_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 100])\n",
      "torch.Size([2000, 100, 1])\n"
     ]
    }
   ],
   "source": [
    "src1 = src_.unsqueeze(-1)\n",
    "\n",
    "print(src_.shape)\n",
    "print(src1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 100, 512])\n",
      "torch.Size([1925, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "src_left_ = pre_linear1(src1[:single_eval_position]) # \n",
    "src_right_ = pre_linear2(src1[single_eval_position:]) # <- linear layers\n",
    "\n",
    "print(src_left_.shape)\n",
    "print(src_right_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_left_ = inter_feature_attn_1(src_left_, src_left_, src_left_)[0] #\n",
    "src_right_ = inter_feature_attn_2(src_right_, src_right_, src_right_)[0] # <- interfeature attnetion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer NEW 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=512\n",
    "dropout=0.0\n",
    "layer_norm_eps=1e-5\n",
    "device=None\n",
    "dtype=None\n",
    "\n",
    "batch_first=False\n",
    "factory_kwargs = {'device': device, 'dtype': dtype}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first)\n",
    "\n",
    "############################## Inter-feature attention ############################################\n",
    "pre_linear1 = Linear(1, d_model)\n",
    "pre_linear2 = Linear(d_model, 1)\n",
    "\n",
    "inter_feature_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                    **factory_kwargs)\n",
    "\n",
    "pre_norm_ = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "pre_dropout = Dropout(dropout)\n",
    "####################################################################################################\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "# src = torch.rand(2000,emsize) # input that's originally comming \n",
    "src = torch.rand(100,200,100)\n",
    "src_mask = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_ = src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "################### The interfeature implementation ###########################\n",
    "            \n",
    "src_left_ = src_[:single_eval_position]\n",
    "src_right_ = src_[single_eval_position:] # <- split the data\n",
    "\n",
    "src_left_ = rearrange(src_left_, 'b h w -> w (b h) 1') #\n",
    "src_right_ = rearrange(src_right_, 'b h w -> w (b h) 1') # <- rearrange for Interfeature attention\n",
    "\n",
    "print(f'Before 1st linear layer dimensions of src_left_ and right_: {src_left_.size(), src_right_.size()}')\n",
    "\n",
    "src_left_ = self.pre_linear1(src_left_) # <- linear layers\n",
    "src_right_ = self.pre_linear1(src_right_) # <- linear layers\n",
    "\n",
    "print(f'After 1st linear layer dimensions of src_left_ and right_: {src_left_.size(), src_right_.size()}')\n",
    "\n",
    "src_left_ = self.inter_feature_attn(src_left_, src_left_, src_left_)[0] #\n",
    "src_right_ = self.inter_feature_attn(src_right_, src_right_, src_right_)[0] # <- interfeature attnetion\n",
    "\n",
    "print(f'After 1st attention layer dimensions of src_left_ and right_: {src_left_.size(), src_right_.size()}')\n",
    "\n",
    "src_left_ = self.pre_linear2(src_left_) # \n",
    "src_right_ = self.pre_linear2(src_right_) # <- linear layers to squeeze everything back up\n",
    "\n",
    "print(f'After 2st linear layer dimensions of src_left_ and right_: {src_left_.size(), src_right_.size()}')\n",
    "\n",
    "src_left_ = rearrange(src_left_, 'w (b h) 1 -> b h w', b = single_eval_position)\n",
    "src_right_ = rearrange(src_right_, 'w (b h) 1 -> b h w', b = src_.size()[0] - single_eval_position)\n",
    "\n",
    "print(f'After squeeze dimensions of src_left_ and right_: {src_left_.size(), src_right_.size()}')\n",
    "\n",
    "src_left_ = self.pre_norm_(src_[:single_eval_position] + self.pre_dropout(src_left_))\n",
    "src_right_ = self.pre_norm_(src_[single_eval_position:] + self.pre_dropout(src_right_)) # <- residual layer\n",
    "###############################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 200, 100])\n",
      "torch.Size([25, 200, 100])\n"
     ]
    }
   ],
   "source": [
    "src_left_ = src_[:single_eval_position]\n",
    "src_right_ = src_[single_eval_position:] # <- split the data\n",
    "\n",
    "print(src_left_.shape)\n",
    "print(src_right_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 15000, 1])\n",
      "torch.Size([100, 5000, 1])\n"
     ]
    }
   ],
   "source": [
    "src_left_ = rearrange(src_left_, 'b h w -> w (b h) 1') #\n",
    "src_right_ = rearrange(src_right_, 'b h w -> w (b h) 1') # <- rearrange for Interfeature attention\n",
    "\n",
    "print(src_left_.shape)\n",
    "print(src_right_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 9, 4, 8],\n",
      "        [3, 3, 1, 1],\n",
      "        [9, 2, 8, 9],\n",
      "        [6, 3, 3, 0],\n",
      "        [2, 1, 2, 6]])\n",
      "tensor([[[5],\n",
      "         [9],\n",
      "         [4],\n",
      "         [8]],\n",
      "\n",
      "        [[3],\n",
      "         [3],\n",
      "         [1],\n",
      "         [1]],\n",
      "\n",
      "        [[9],\n",
      "         [2],\n",
      "         [8],\n",
      "         [9]],\n",
      "\n",
      "        [[6],\n",
      "         [3],\n",
      "         [3],\n",
      "         [0]],\n",
      "\n",
      "        [[2],\n",
      "         [1],\n",
      "         [2],\n",
      "         [6]]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "tensor = torch.randint(0,10,[5,4])\n",
    "linear = Linear(1, 3)\n",
    "\n",
    "print(tensor)\n",
    "print(tensor.unsqueeze(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 3])\n",
      "tensor([[[-2.2934, -3.3403, -0.7902],\n",
      "         [-3.7830, -5.7561, -1.4607],\n",
      "         [-1.9210, -2.7363, -0.6226],\n",
      "         [-3.4106, -5.1521, -1.2930]],\n",
      "\n",
      "        [[-1.5486, -2.1323, -0.4550],\n",
      "         [-1.5486, -2.1323, -0.4550],\n",
      "         [-0.8037, -0.9244, -0.1197],\n",
      "         [-0.8037, -0.9244, -0.1197]],\n",
      "\n",
      "        [[-3.7830, -5.7561, -1.4607],\n",
      "         [-1.1762, -1.5284, -0.2873],\n",
      "         [-3.4106, -5.1521, -1.2930],\n",
      "         [-3.7830, -5.7561, -1.4607]],\n",
      "\n",
      "        [[-2.6658, -3.9442, -0.9578],\n",
      "         [-1.5486, -2.1323, -0.4550],\n",
      "         [-1.5486, -2.1323, -0.4550],\n",
      "         [-0.4313, -0.3204,  0.0479]],\n",
      "\n",
      "        [[-1.1762, -1.5284, -0.2873],\n",
      "         [-0.8037, -0.9244, -0.1197],\n",
      "         [-1.1762, -1.5284, -0.2873],\n",
      "         [-2.6658, -3.9442, -0.9578]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(linear(tensor.unsqueeze(-1).float()).shape)\n",
    "print(linear(tensor.unsqueeze(-1).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "x = linear(tensor.unsqueeze(-1).float())\n",
    "query, key, value = x, x, x\n",
    "\n",
    "print(query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "query, key, value = [x.transpose(1, 0) for x in (query, key, value)]\n",
    "\n",
    "print(query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4],\n",
       "        [9],\n",
       "        [3],\n",
       "        [0],\n",
       "        [3]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.randint(0,10,[1152, 1])\n",
    "x[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 6])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "tensor1 = torch.rand([5,1,6])\n",
    "tensor2 = torch.rand([5,6])\n",
    "#tensor1 = torch.randint(0,10,[5,1,6])\n",
    "\n",
    "print(tensor2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 6])\n"
     ]
    }
   ],
   "source": [
    "query, key, value = tensor1, tensor1, tensor1\n",
    "multihead_attn = nn.MultiheadAttention(6, 2)\n",
    "attn_output1, attn_output_weights = multihead_attn(query, key, value)\n",
    "\n",
    "print(attn_output1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'exho'\n"
     ]
    }
   ],
   "source": [
    "%%script exho skipping\n",
    "\n",
    "query2, key2, value2 = tensor2, tensor2, tensor2\n",
    "multihead_attn2 = nn.MultiheadAttention(6, 2)\n",
    "attn_output2, attn_output_weights2 = multihead_attn(query2, key2, value2)\n",
    "\n",
    "print(attn_output2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 1])\n",
      "tensor([[[5],\n",
      "         [9],\n",
      "         [4]],\n",
      "\n",
      "        [[8],\n",
      "         [3],\n",
      "         [3]],\n",
      "\n",
      "        [[1],\n",
      "         [1],\n",
      "         [9]],\n",
      "\n",
      "        [[2],\n",
      "         [8],\n",
      "         [9]]])\n",
      "torch.Size([4, 3])\n",
      "tensor([[5, 9, 4],\n",
      "        [8, 3, 3],\n",
      "        [1, 1, 9],\n",
      "        [2, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "tensor = torch.randint(0,10,[4,3,1])\n",
    "print(tensor.shape)\n",
    "print(tensor)\n",
    "print(tensor.squeeze(2).shape)\n",
    "print(tensor.squeeze(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e0db07081617c9a6eca3dcd9ae8397d12b2119e3e97abe44125c2c5f990a5fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
